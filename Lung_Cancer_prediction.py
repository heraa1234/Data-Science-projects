# -*- coding: utf-8 -*-
"""Krunatic_Lung_cancer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cm7Qn9PRVIvc1UMZmsceRxo4I6hVDOaz
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import Ridge, Lasso
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, recall_score, precision_score
import xgboost as xgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM, Dropout

data = pd.read_csv('/content/survey lung cancer.csv')
data.head()

print(data.info())

le_gender = LabelEncoder()
le_lung_cancer = LabelEncoder()
data['GENDER'] = le_gender.fit_transform(data['GENDER'])
data['LUNG_CANCER'] = le_lung_cancer.fit_transform(data['LUNG_CANCER'])

plt.figure(figsize=(15, 10))
for i, column in enumerate(data.columns[:-1], 1):
    plt.subplot(4, 4, i)
    sns.histplot(data[column], kde=True)
    plt.title(f'Distribution of {column}')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

X = data.drop('LUNG_CANCER', axis=1)
y = data['LUNG_CANCER']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

ridge = Ridge()
ridge.fit(X_train, y_train)
y_pred_ridge = (ridge.predict(X_test) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred_ridge)
f1 = f1_score(y_test, y_pred_ridge,average='weighted')
recall = recall_score(y_test, y_pred_ridge,average='weighted')
precision = precision_score(y_test, y_pred_ridge,average='weighted')
print(classification_report(y_test, y_pred_ridge))
cm = confusion_matrix(y_test, y_pred_ridge)
print("Ridge Regression")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print("Confusion Matrix:")
print(cm)
print("\n")

lasso = Lasso()
lasso.fit(X_train, y_train)
y_pred_lasso = (lasso.predict(X_test) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred_lasso)
f1 = f1_score(y_test, y_pred_lasso,average='weighted')
recall = recall_score(y_test, y_pred_lasso,average='weighted')
precision = precision_score(y_test, y_pred_lasso,average='weighted')
print(classification_report(y_test, y_pred_lasso))
cm = confusion_matrix(y_test, y_pred_lasso)
print("Lasso Regression")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print("Confusion Matrix:")
print(cm)
print("\n")

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
accuracy = accuracy_score(y_test, y_pred_knn)
f1 = f1_score(y_test, y_pred_knn,average='weighted')
recall = recall_score(y_test, y_pred_knn,average='weighted')
precision = precision_score(y_test, y_pred_knn,average='weighted')
print(classification_report(y_test, y_pred_knn))
cm = confusion_matrix(y_test, y_pred_knn)
print("Knn classification")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print("Confusion Matrix:")
print(cm)
print("\n")

svm = SVC()
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)
accuracy = accuracy_score(y_test, y_pred_svm)
f1 = f1_score(y_test, y_pred_svm,average='weighted')
recall = recall_score(y_test, y_pred_svm,average='weighted')
precision = precision_score(y_test, y_pred_svm,average='weighted')
print(classification_report(y_test, y_pred_svm))
cm = confusion_matrix(y_test, y_pred_svm)
print("Support Vector Machine")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print("Confusion Matrix:")
print(cm)
print("\n")

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred_rf)
f1 = f1_score(y_test, y_pred_rf,average='weighted')
recall = recall_score(y_test, y_pred_rf,average='weighted')
precision = precision_score(y_test, y_pred_rf,average='weighted')
print(classification_report(y_test, y_pred_rf))
cm = confusion_matrix(y_test, y_pred_rf)
print("Random Forest")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print("Confusion Matrix:")
print(cm)
print("\n")

ada = AdaBoostClassifier()
ada.fit(X_train, y_train)
y_pred_ada = ada.predict(X_test)
accuracy = accuracy_score(y_test, y_pred_ada)
f1 = f1_score(y_test, y_pred_ada,average='weighted')
recall = recall_score(y_test, y_pred_ada,average='weighted')
precision = precision_score(y_test, y_pred_ada,average='weighted')
print(classification_report(y_test, y_pred_ada))
cm = confusion_matrix(y_test, y_pred_ada)
print("AdaBoost")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print("Confusion Matrix:")
print(cm)
print("\n")

gb = GradientBoostingClassifier()
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)
accuracy = accuracy_score(y_test, y_pred_gb)
f1 = f1_score(y_test, y_pred_gb,average='weighted')
recall = recall_score(y_test, y_pred_gb,average='weighted')
precision = precision_score(y_test, y_pred_gb,average='weighted')
print(classification_report(y_test, y_pred_gb))
cm = confusion_matrix(y_test, y_pred_gb)
print("Gradient Boosting")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print("Confusion Matrix:")
print(cm)
print("\n")

cnn = Sequential([
    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),
    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
cnn.fit(X_train, y_train, epochs=20, batch_size=16, verbose=1)
y_pred_cnn = (cnn.predict(X_test) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred_cnn)
f1 = f1_score(y_test, y_pred_cnn,average='weighted')
recall = recall_score(y_test, y_pred_cnn,average='weighted')
precision = precision_score(y_test, y_pred_cnn,average='weighted')
print(classification_report(y_test, y_pred_cnn))
cm = confusion_matrix(y_test, y_pred_cnn)
print("Convolutional Neural Network")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print("Confusion Matrix:")
print(cm)
print("\n")

rnn = Sequential([
    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=True),
    LSTM(32),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
rnn.fit(X_train, y_train, epochs=20, batch_size=16, verbose=1)
y_pred_rnn = (rnn.predict(X_test) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred_rnn)
f1 = f1_score(y_test, y_pred_rnn,average='weighted')
recall = recall_score(y_test, y_pred_rnn,average='weighted')
precision = precision_score(y_test, y_pred_rnn,average='weighted')
print(classification_report(y_test, y_pred_rnn))
cm = confusion_matrix(y_test, y_pred_rnn)
print("Recurrent Neural Network")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print("Confusion Matrix:")
print(cm)
print("\n")

models = ['Ridge Regression', 'Lasso Regression', 'KNN', 'SVM', 'Random Forest', 'AdaBoost', 'Gradient Boosting','Concolutional Neural Network','Recurrent Neural Network']
accuracies = [
    accuracy_score(y_test, y_pred_ridge), accuracy_score(y_test, y_pred_lasso),
    accuracy_score(y_test, y_pred_knn), accuracy_score(y_test, y_pred_svm),
    accuracy_score(y_test, y_pred_rf), accuracy_score(y_test, y_pred_ada),
    accuracy_score(y_test, y_pred_gb),accuracy_score(y_test, y_pred_cnn),accuracy_score(y_test, y_pred_rnn)
]

plt.figure(figsize=(12, 6))
sns.barplot(x=models, y=accuracies)
plt.title('Model Accuracy Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.show()

"""***1.Model Comparision Report***
This report evaluates various machine learning algorithms to predict lung cancer based on health and lifestyle factors. The dataset consists of 309 samples with a mix of categorical and numerical features, and the target variable, LUNG_CANCER, indicates whether a person has lung cancer (1 for “Yes” and 0 for “No”).
Based on the performance metrics, ADA boost is recommended for production deployment. XGBoost has:

The highest accuracy (98%) among evaluated models.
Balanced precision and recall scores, which are critical for minimizing false
positives and false negatives.
High training speed and scalability, which makes it suitable for larger datasets in a production setting.
CNN and RNN could also be considered for production due to its robustness and slightly lower computational cost. However, Adaboost’s superior performance in accuracy and balanced metrics makes it the preferred choice.

**3. Data Challenges Faced**

*3.1. Categorical Data Encoding*

Challenge: The dataset contained categorical variables, such as GENDER and LUNG_CANCER, which needed to be converted into numerical format for the machine learning models to process them.

Technique Used:

Label Encoding: We used LabelEncoder to convert categorical variables (GENDER and LUNG_CANCER) into numerical labels (0 and 1).

Reasoning: Label encoding was selected because these categorical variables have a clear ordinal relationship (e.g., 'Male' = 1, 'Female' = 0), and machine learning models require numerical input. Label encoding was appropriate for the GENDER column, as the categorical variables don't represent arbitrary labels but actual distinctions.

*3.2. Feature Scaling*

Challenge: Many machine learning models, such as K-Nearest Neighbors (KNN) and Support Vector Machines (SVM), are sensitive to the scale of the data. Some features may have a large range of values, which could dominate the model's learning process and lead to poor performance.

Technique Used:

Standardization (Z-Score Scaling): We used StandardScaler to scale the features to have a mean of 0 and a standard deviation of 1.

Reasoning: Standardization was chosen because the data contains features with different units and ranges. Standardization ensures that all features contribute equally to the model's learning process, particularly for distance-based algorithms like KNN and SVM.

*3.3. Class Imbalance*

Challenge: The dataset may suffer from class imbalance, meaning there may be more samples of one class (e.g., "No cancer") than the other (e.g., "Cancer"), leading to biased model predictions.

Technique Used:

Resampling or Adjusting Class Weights: While this specific dataset did not require significant resampling, we took care to monitor the model’s precision, recall, and F1-score to ensure balanced performance across both classes. Additionally, for all models, we can use class weights to give more importance to the minority class (lung cancer).

Reasoning: This technique ensures that the model does not simply predict the majority class all the time, which would lead to misleading accuracy results. By focusing on metrics like precision and recall, we ensure that both classes are properly represented.

*4. Summary of Techniques Used:*

To address the challenges in this dataset, the following techniques were employed:

Label Encoding: Applied to convert categorical variables into numerical format for machine learning algorithms.

Standardization: Scaled the features to ensure that all models performed optimally, especially distance-based models like KNN and SVM.

Class Imbalance Handling: Used class weights to balance the model's performance on both classes, ensuring reliable predictions.

These are the challenges I faced through the ongoing process of code
"""

