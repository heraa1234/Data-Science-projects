# -*- coding: utf-8 -*-
"""Real time sales analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11-dFfrQTQmVK7QxB4shA3Tp8czDMmzR-
"""

# Import Required Libraries
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum, avg

def get_data_from_db():
    data=pd.read_csv("/content/simulated_sales_data.csv")
    return data

# Step 2: Real-Time Simulation of Data
def simulate_real_time_data(data, batch_size=100):
    for i in range(0, len(data), batch_size):
        yield data.iloc[i:i+batch_size]

# Step 3: Data Cleaning
def clean_data(df):
  df['Sales'].fillna(df['Sales'].mean(), inplace=True)
  df['Profit'].fillna(df['Profit'].mean(), inplace=True)
  df['Profit Margin'] = df['Profit'] / df['Sales']
  return df

data=get_data_from_db()

data["Category"].replace(["Furniture","Books","Toys","Clothing","Electronics"],[0,1,2,3,4],inplace=True)
data["Region"].replace(["East","West","Central","North","South"],[0,1,2,3,4],inplace=True)

# Step 4: Exploratory Data Analysis
def perform_eda(df):
    print("Summary Statistics:\n", df.describe())
    print("\nCorrelation Matrix:\n", df.corr())
    sns.heatmap(df.corr(), annot=True)
    plt.show()

# Step 5: Data Visualization
def visualize_data(df):
    plt.figure(figsize=(10, 6))
    sns.barplot(data=df, x='Region', y='Sales', ci=None, estimator=np.sum)
    plt.title('Sales by Region')
    plt.show()

    plt.figure(figsize=(10, 6))
    sns.lineplot(data=df, x='Region', y='Profit', estimator=np.mean)
    plt.title('Average Profit Over Time')
    plt.show()

# Step 6: Advanced Analysis - Clustering
def perform_clustering(df):
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df[['Sales', 'Profit']])
    kmeans = KMeans(n_clusters=3, random_state=42)
    df['Cluster'] = kmeans.fit_predict(df_scaled)
    sns.scatterplot(data=df, x='Sales', y='Profit', hue='Cluster')
    plt.title('Customer Segmentation')
    plt.show()

# Step 7: Real-Time Data Analysis (Streaming)
def real_time_analysis(data_stream):
    spark = SparkSession.builder.appName("RealTimeSalesAnalysis").getOrCreate()
    schema = "Order_ID INT, Product STRING, Category STRING, Region STRING, Order_Date TIMESTAMP, Sales DOUBLE, Quantity INT, Discount DOUBLE, Profit DOUBLE, Customer_ID STRING"
    streaming_df = spark.readStream.schema(schema).csv("path/to/streaming/data")

    analysis = (
        streaming_df.groupBy("Region")
        .agg(
            _sum("Sales").alias("Total Sales"),
            avg("Profit").alias("Average Profit")
        )
    )

    query = analysis.writeStream.format("console").outputMode("complete").start()
    query.awaitTermination()

# Step 8: Actionable Insights and Reporting
def generate_insights(df):
    high_sales_regions = df.groupby('Region')['Sales'].sum().sort_values(ascending=False)
    print("Top Performing Regions:\n", high_sales_regions)

    top_products = df.groupby('Product ID')['Sales'].sum().nlargest(10)
    print("Top 10 Products by Sales:\n", top_products)

# Load Data
sales_data = get_data_from_db()

sales_data["Category"].replace(["Furniture","Books","Toys","Clothing","Electronics"],[0,1,2,3,4],inplace=True)
sales_data["Region"].replace(["East","West","Central","North","South"],[0,1,2,3,4],inplace=True)

# Main Function
if __name__ == "__main__":


    # Simulate and Process Real-Time Data
    for batch in simulate_real_time_data(sales_data):
        cleaned_data = clean_data(batch)
        perform_eda(cleaned_data)
        visualize_data(cleaned_data)
        perform_clustering(cleaned_data)
        generate_insights(cleaned_data)

